{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances as dist, euclidean_distances as dist2\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from models.model_linear import Linearnet\n",
    "from models.model_mlp import Mlp\n",
    "from models.model_cnn import Cnn\n",
    "from models.model_resnet import Resnet\n",
    "from utils.utils_data import generate_real_dataloader\n",
    "from utils.utils_data import prepare_cv_datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import savemat\n",
    "\n",
    "from utils.utils_data import generate_synthetic_hypercube_data\n",
    "from synthetic_classification_generator import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Partial Label Generation: based on the “Madelon” dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial: \n",
      " [[0. 1. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 1. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 0. 0. 0.]]\n",
      "true: \n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "datadir = \"./data/realworld/\"\n",
    "num_classes = 10\n",
    "num_samples = 1000\n",
    "num_features = 5\n",
    "class_sep = 1.0\n",
    "partial_rate = 0.3\n",
    "data_seed = 42\n",
    "\n",
    "# n_features = informative + redundant + repeated + random/useless\n",
    "X, y, centroids, y_centroids = make_classification(n_samples=num_samples,\n",
    "                                                        n_features=num_features, \n",
    "                                                        n_informative=num_features, # all features are informative\n",
    "                                                        n_redundant=0,\n",
    "                                                        n_repeated=0,\n",
    "                                                        n_classes=num_classes,\n",
    "                                                        n_clusters_per_class=1, # each class is associated with a single cluster\n",
    "                                                        flip_y=0.00,\n",
    "                                                        class_sep=class_sep,    # default 1.0\n",
    "                                                        hypercube=True,\n",
    "                                                        shift=0.0,\n",
    "                                                        scale=1.0,\n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=data_seed,\n",
    "                                                        return_centroids=True)\n",
    "\n",
    "y_bin = np.zeros((num_samples, num_classes))\n",
    "y_bin[np.arange(y.size), y] = 1\n",
    "\n",
    "## Generate Partial Label\n",
    "partial_y = np.zeros((num_samples, num_classes))\n",
    "partial_y[np.arange(y.size), y] = 1\n",
    "num_distractors = int(partial_rate*num_classes)\n",
    "sample_centroid_distances = dist(X, Y=centroids)\n",
    "for x in range(num_samples):\n",
    "    candidate_distractors_sorted = list(np.argsort(sample_centroid_distances[x]))\n",
    "    distractors = candidate_distractors_sorted[:num_distractors]\n",
    "    # if the true label is selected among the distractors, replace with additional distractor\n",
    "    if y[x] in distractors:\n",
    "        distractors.append(candidate_distractors_sorted[num_distractors-1])\n",
    "    partial_y[x, distractors] = 1\n",
    "\n",
    "print(\"partial: \\n\", partial_y[:10])\n",
    "print(\"true: \\n\", y_bin[:10])\n",
    "\n",
    "## Save Dataset \n",
    "dt = dict()\n",
    "dt['features'] = X\n",
    "dt['p_labels'] = partial_y\n",
    "dt['logitlabels'] = y_bin\n",
    "\n",
    "datapath = os.path.join(datadir, f'synthetic_{num_samples}_{num_classes}_{num_features}_{partial_rate}_{class_sep}_{data_seed}.mat')\n",
    "savemat(datapath, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "num_samples = 1000\n",
    "num_features = 5\n",
    "class_sep = [0.5, 1.0, 1.5, 2.0]\n",
    "data_seed = 42\n",
    "\n",
    "for csep in class_sep:\n",
    "    # n_features = informative + redundant + repeated + random/useless\n",
    "    X, y, centroids, y_centroids = make_classification(n_samples=num_samples,\n",
    "                                                        n_features=num_features, \n",
    "                                                        n_informative=num_features, # all features are informative\n",
    "                                                        n_redundant=0,\n",
    "                                                        n_repeated=0,\n",
    "                                                        n_classes=num_classes,\n",
    "                                                        n_clusters_per_class=1, # each class is associated with a single cluster\n",
    "                                                        flip_y=0.00,\n",
    "                                                        class_sep=csep,    # default 1.0\n",
    "                                                        hypercube=True,\n",
    "                                                        shift=0.0,\n",
    "                                                        scale=1.0,\n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=data_seed,\n",
    "                                                        return_centroids=True)\n",
    "    n_centroids = y_centroids.shape\n",
    "    Xcomplete = np.vstack((X, centroids))\n",
    "    ycomplete = np.concatenate((y, np.ones(n_centroids)*(-1)))\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(Xcomplete)\n",
    "    X2d = pca.transform(Xcomplete)\n",
    "\n",
    "    outfile = \"figs/synthetic_{}_{}_{}_{}_{}.png\".format(num_samples, num_classes, num_features, csep, data_seed)\n",
    "    plt.scatter(X2d[:num_samples,0], X2d[:num_samples,1], s=1, c=ycomplete[:num_samples], cmap=plt.get_cmap(\"tab10\"), marker='o')\n",
    "    plt.scatter(X2d[num_samples:,0], X2d[num_samples:,1], s=5, color='black', marker='*')\n",
    "    plt.savefig(outfile)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM-based Synthetic Partial Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    \"\"\"\n",
    "    Draws the ellipses for each of the predicted cluster\n",
    "    \"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "C = 10\n",
    "std = 0.9\n",
    "N = 10\n",
    "d = 10\n",
    "plot = False\n",
    "\n",
    "## Making a synthetic dataset with 4 clusters, and number of samples is 400.\n",
    "X_base, y_base = make_blobs(n_samples=100*C, n_features=d, centers=C, cluster_std=std, random_state=0)\n",
    "X = X_base[:, ::-1]\n",
    "\n",
    "## Fit (and plot) a GMM on the synthetic dataset\n",
    "gmm = GMM(n_components=C, random_state=42)\n",
    "gmm.fit(X_base)\n",
    "if plot:    \n",
    "    plot_gmm(gmm, X_base)\n",
    "\n",
    "## Sample from the fitted GMM\n",
    "X, y = gmm.sample(N)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering-based Partial Label Generation for Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dset = 'cifar10'\n",
    "B = 100\n",
    "\n",
    "if dset in ['mnist', 'kmnist', 'fashion', 'cifar10']:\n",
    "    (full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, K) = prepare_cv_datasets(dataname=dset, batch_size=B)\n",
    "\n",
    "for i, (data, labels) in enumerate(full_train_loader):\n",
    "    K = torch.max(\n",
    "        labels\n",
    "    ) + 1  # K is number of classes, full_train_loader is full batch\n",
    "    N,c,row,col = data.shape\n",
    "\n",
    "flattened_data = data.reshape((N, c*row*col))\n",
    "flattened_data_plus_label = torch.cat((flattened_data.reshape((c*row*col, N)), labels.unsqueeze(0))).reshape(N, c*row*col+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n",
      "Number of clusters:  10\n",
      "(50000, 3072)\n",
      "[6 3 0 ... 1 3 1]\n",
      "[1 6 6 ... 3 0 1]\n",
      "[[1.   0.11 0.11 0.1  0.11 0.12 0.13 0.11 0.09 0.11]\n",
      " [0.1  1.   0.12 0.11 0.12 0.13 0.13 0.11 0.08 0.1 ]\n",
      " [0.1  0.12 1.   0.1  0.11 0.13 0.12 0.11 0.09 0.11]\n",
      " [0.1  0.12 0.11 1.   0.12 0.12 0.11 0.11 0.09 0.12]\n",
      " [0.1  0.12 0.11 0.1  1.   0.12 0.12 0.12 0.09 0.12]\n",
      " [0.1  0.12 0.12 0.1  0.12 1.   0.12 0.11 0.09 0.12]\n",
      " [0.12 0.12 0.12 0.1  0.11 0.12 1.   0.11 0.1  0.12]\n",
      " [0.11 0.12 0.12 0.1  0.12 0.12 0.12 1.   0.09 0.11]\n",
      " [0.1  0.1  0.11 0.11 0.11 0.12 0.13 0.11 1.   0.11]\n",
      " [0.1  0.1  0.12 0.11 0.12 0.13 0.13 0.11 0.09 1.  ]]\n",
      "Ambiguity degree:  0.13492047614284286\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes: \", K.item())\n",
    "num_clusters = 1*K.item()\n",
    "print(\"Number of clusters: \", num_clusters)\n",
    "X = flattened_data.numpy()\n",
    "print(X.shape)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=1).fit(X)\n",
    "print(kmeans.labels_)\n",
    "print(labels.numpy())\n",
    "\n",
    "# confusion_labels = {}\n",
    "# confusion_labels.update([(cluster,set()) for cluster in range(num_clusters)])\n",
    "# for i,cluster in enumerate(kmeans.labels_):\n",
    "#     true_label_i = labels[i].item()\n",
    "#     confusion_labels[cluster].add(true_label_i)\n",
    "\n",
    "# for cluster in confusion_labels.keys():\n",
    "#     print(f\"Cluster {cluster} Candidate Labels {confusion_labels[cluster]}\")\n",
    "\n",
    "sample_size = int(N*0.01) # 1% \n",
    "sample = random.sample(list(range(N)), sample_size)\t\n",
    "confusion_labels = np.eye(K)\n",
    "for i,cluster_i in enumerate(kmeans.labels_[sample]):\n",
    "    for j,cluster_j in enumerate(kmeans.labels_):\n",
    "        if cluster_i==cluster_j:\n",
    "            true_label_i = labels[i].item()\n",
    "            true_label_j = labels[j].item()\n",
    "            if true_label_i!=true_label_j:\n",
    "                confusion_labels[true_label_i, true_label_j] += 1\n",
    "                confusion_labels[true_label_j, true_label_i] += 1\n",
    "\n",
    "# normalize to get probs\n",
    "confusion_labels = normalize(confusion_labels, axis=1, norm='l1')\n",
    "np.fill_diagonal(confusion_labels, 1.0)\n",
    "print(np.around(confusion_labels, 2))\n",
    "print(\"Ambiguity degree: \", confusion_labels[confusion_labels<1.0].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('neuro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45a9a558b30b86d9f732c54dbfd32f3dc135cf8debc1699b6136107345de1818"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
