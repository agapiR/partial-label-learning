{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances as dist, euclidean_distances as dist2\n",
    "import torch\n",
    "from models.model_linear import Linearnet\n",
    "from models.model_mlp import Mlp\n",
    "from models.model_cnn import Cnn\n",
    "from models.model_resnet import Resnet\n",
    "from utils.utils_data import generate_real_dataloader\n",
    "from utils.utils_data import prepare_cv_datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Partial Label Generation: based on the “Madelon” dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_classification_generator import make_classification\n",
    "\n",
    "datadir = \"./data/realworld/\"\n",
    "num_classes = 10\n",
    "num_samples = 1000\n",
    "feature_dim = 150\n",
    "distractor_ratios = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "for distractor_ratio in distractor_ratios:\n",
    "    num_distractors = int(distractor_ratio*num_classes)\n",
    "\n",
    "    # n_features = informative + redundant + repeated + random/useless\n",
    "\n",
    "    X, y_true, centroids = make_classification(n_samples=num_samples,\n",
    "                                            n_features=feature_dim, \n",
    "                                            n_informative=feature_dim, # all features are informative\n",
    "                                            n_redundant=0,\n",
    "                                            n_repeated=0,\n",
    "                                            n_classes=num_classes,\n",
    "                                            n_clusters_per_class=1, # each class is associated with a single cluster\n",
    "                                            flip_y=0.01,\n",
    "                                            class_sep=1.0,          # default 1.0\n",
    "                                            hypercube=True,\n",
    "                                            shift=0.0,\n",
    "                                            scale=1.0,\n",
    "                                            shuffle=False,\n",
    "                                            random_state=None,\n",
    "                                            return_centroids=True)\n",
    "\n",
    "    # print('features', X.shape, X.dtype)\n",
    "    # print('logitlabels', y_true.shape, y_true.dtype)\n",
    "\n",
    "    y = partial_y = np.zeros((num_samples, num_classes))\n",
    "    y[np.arange(y_true.size), y_true] = 1\n",
    "    partial_y[np.arange(y_true.size), y_true] = 1\n",
    "\n",
    "    ## Generate Partial Label\n",
    "    # print('centroids', centroids.shape)\n",
    "    sample_centroid_distances = dist(X, Y=centroids)\n",
    "    for x in range(num_samples):\n",
    "        distractor_weights = sample_centroid_distances[x]\n",
    "        distractor_weights_norm = sample_centroid_distances[x] / sample_centroid_distances[x].sum()\n",
    "        # print(f'sample {x} - norm centroid distances {np.around(distractor_weights,4)} - true label {y_true[x]} - argsort {np.argsort(distractor_weights)}')\n",
    "        num_partial_labels = random.randint(1, num_distractors+1)\n",
    "        partial_y[x, np.argsort(distractor_weights)[0:num_partial_labels]] = 1\n",
    "        # print(partial_y[x])\n",
    "        \n",
    "    dt = dict()\n",
    "    dt['features'] = X\n",
    "    dt['p_labels'] = partial_y\n",
    "    dt['logitlabels'] = y\n",
    "\n",
    "    datapath = os.path.join(datadir, \"synthetic-{}.mat\".format(distractor_ratio))\n",
    "    savemat(datapath, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM-based Synthetic Partial Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    \"\"\"\n",
    "    Draws the ellipses for each of the predicted cluster\n",
    "    \"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "C = 10\n",
    "std = 0.9\n",
    "N = 10\n",
    "d = 10\n",
    "\n",
    "## Making a synthetic dataset with 4 clusters, and number of samples is 400.\n",
    "X_base, y_base = make_blobs(n_samples=100*C, n_features=d, centers=C, cluster_std=std, random_state=0)\n",
    "X = X_base[:, ::-1]\n",
    "\n",
    "## Fit (and plot) a GMM on the synthetic dataset\n",
    "gmm = GMM(n_components=C, random_state=42)\n",
    "gmm.fit(X_base)\n",
    "# plot_gmm(gmm, X_base)\n",
    "\n",
    "## Sample from the fitted GMM\n",
    "X, y = gmm.sample(N)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering-based Partial Label Generation for Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dset = 'cifar10'\n",
    "B = 100\n",
    "\n",
    "if dset in ['mnist', 'kmnist', 'fashion', 'cifar10']:\n",
    "    (full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, K) = prepare_cv_datasets(dataname=dset, batch_size=B)\n",
    "\n",
    "for i, (data, labels) in enumerate(full_train_loader):\n",
    "    K = torch.max(\n",
    "        labels\n",
    "    ) + 1  # K is number of classes, full_train_loader is full batch\n",
    "    N,c,row,col = data.shape\n",
    "\n",
    "flattened_data = data.reshape((N, c*row*col))\n",
    "flattened_data_plus_label = torch.cat((flattened_data.reshape((c*row*col, N)), labels.unsqueeze(0))).reshape(N, c*row*col+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  10\n",
      "Number of clusters:  10\n",
      "(50000, 3072)\n",
      "[6 3 0 ... 1 3 1]\n",
      "[1 6 6 ... 3 0 1]\n",
      "[[1.   0.11 0.11 0.1  0.11 0.12 0.13 0.11 0.09 0.11]\n",
      " [0.1  1.   0.12 0.11 0.12 0.13 0.13 0.11 0.08 0.1 ]\n",
      " [0.1  0.12 1.   0.1  0.11 0.13 0.12 0.11 0.09 0.11]\n",
      " [0.1  0.12 0.11 1.   0.12 0.12 0.11 0.11 0.09 0.12]\n",
      " [0.1  0.12 0.11 0.1  1.   0.12 0.12 0.12 0.09 0.12]\n",
      " [0.1  0.12 0.12 0.1  0.12 1.   0.12 0.11 0.09 0.12]\n",
      " [0.12 0.12 0.12 0.1  0.11 0.12 1.   0.11 0.1  0.12]\n",
      " [0.11 0.12 0.12 0.1  0.12 0.12 0.12 1.   0.09 0.11]\n",
      " [0.1  0.1  0.11 0.11 0.11 0.12 0.13 0.11 1.   0.11]\n",
      " [0.1  0.1  0.12 0.11 0.12 0.13 0.13 0.11 0.09 1.  ]]\n",
      "Ambiguity degree:  0.13492047614284286\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes: \", K.item())\n",
    "num_clusters = 1*K.item()\n",
    "print(\"Number of clusters: \", num_clusters)\n",
    "X = flattened_data.numpy()\n",
    "print(X.shape)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=1).fit(X)\n",
    "print(kmeans.labels_)\n",
    "print(labels.numpy())\n",
    "\n",
    "# confusion_labels = {}\n",
    "# confusion_labels.update([(cluster,set()) for cluster in range(num_clusters)])\n",
    "# for i,cluster in enumerate(kmeans.labels_):\n",
    "#     true_label_i = labels[i].item()\n",
    "#     confusion_labels[cluster].add(true_label_i)\n",
    "\n",
    "# for cluster in confusion_labels.keys():\n",
    "#     print(f\"Cluster {cluster} Candidate Labels {confusion_labels[cluster]}\")\n",
    "\n",
    "sample_size = int(N*0.01) # 1% \n",
    "sample = random.sample(list(range(N)), sample_size)\t\n",
    "confusion_labels = np.eye(K)\n",
    "for i,cluster_i in enumerate(kmeans.labels_[sample]):\n",
    "    for j,cluster_j in enumerate(kmeans.labels_):\n",
    "        if cluster_i==cluster_j:\n",
    "            true_label_i = labels[i].item()\n",
    "            true_label_j = labels[j].item()\n",
    "            if true_label_i!=true_label_j:\n",
    "                confusion_labels[true_label_i, true_label_j] += 1\n",
    "                confusion_labels[true_label_j, true_label_i] += 1\n",
    "\n",
    "# normalize to get probs\n",
    "confusion_labels = normalize(confusion_labels, axis=1, norm='l1')\n",
    "np.fill_diagonal(confusion_labels, 1.0)\n",
    "print(np.around(confusion_labels, 2))\n",
    "print(\"Ambiguity degree: \", confusion_labels[confusion_labels<1.0].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('neuro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45a9a558b30b86d9f732c54dbfd32f3dc135cf8debc1699b6136107345de1818"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
